{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "k35Swp-8KoKs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: open-spiel in /Users/jameelsaeb/Desktop/cs410/cs410_env/lib/python3.10/site-packages (1.6.9)\n",
      "Requirement already satisfied: numpy>=1.21.5 in /Users/jameelsaeb/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from open-spiel) (2.1.0)\n",
      "Requirement already satisfied: absl-py>=0.10.0 in /Users/jameelsaeb/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from open-spiel) (2.3.1)\n",
      "Requirement already satisfied: scipy>=1.10.1 in /Users/jameelsaeb/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from open-spiel) (1.14.1)\n",
      "Requirement already satisfied: ml-collections>=0.1.1 in /Users/jameelsaeb/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from open-spiel) (1.1.0)\n",
      "Requirement already satisfied: pip>=20.0.2 in /Users/jameelsaeb/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from open-spiel) (22.2.1)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /Users/jameelsaeb/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from open-spiel) (25.4.0)\n",
      "Requirement already satisfied: PyYAML in /Users/jameelsaeb/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from ml-collections>=0.1.1->open-spiel) (6.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /Users/jameelsaeb/Desktop/cs410/cs410_env/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: networkx in /Users/jameelsaeb/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/jameelsaeb/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: jinja2 in /Users/jameelsaeb/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: sympy in /Users/jameelsaeb/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: fsspec in /Users/jameelsaeb/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: filelock in /Users/jameelsaeb/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jameelsaeb/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jameelsaeb/Desktop/cs410/cs410_env/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Needed if running on Colab, comment out if in local environment!\n",
    "!pip3 install open-spiel\n",
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "O1yAh0sTKs3K"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from go_search_problem import GoProblem, GoState\n",
    "from heuristic_go_problems import GoProblemLearnedHeuristic, GoProblemSimpleHeuristic\n",
    "from agents import GreedyAgent, RandomAgent, AlphaBetaAgent\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from game_runner import GameRunner\n",
    "import pickle\n",
    "\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "6XNzHOq6QCQD"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path: str):\n",
    "    with open(path, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    return dataset\n",
    "\n",
    "# We've provided a dataset with pyspiel and without (i.e., pygo)\n",
    "dataset_5x5 = load_dataset('dataset_5x5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "5fAQlSAOLXoj"
   },
   "outputs": [],
   "source": [
    "def save_model(path: str, model):\n",
    "    \"\"\"\n",
    "    Save model to a file\n",
    "    Input:\n",
    "        path: path to save model to\n",
    "        model: Pytorch model to save\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }, path)\n",
    "\n",
    "def load_model(path: str, model):\n",
    "    \"\"\"\n",
    "    Load model from file\n",
    "\n",
    "    Note: you still need to provide a model (with the same architecture as the saved model))\n",
    "\n",
    "    Input:\n",
    "        path: path to load model from\n",
    "        model: Pytorch model to load\n",
    "    Output:\n",
    "        model: Pytorch model loaded from file\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOzaAXYrM4d3"
   },
   "source": [
    "# Task 1: Convert GameState to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "1hbg6LkAMrZW"
   },
   "outputs": [],
   "source": [
    "def get_features(game_state: GoState):\n",
    "    \"\"\"\n",
    "    Map a game state to a list of features.\n",
    "\n",
    "    Some useful functions from game_state include:\n",
    "        game_state.size: size of the board\n",
    "        get_pieces_coordinates(player_index): get coordinates of all pieces of a player (0 or 1)\n",
    "        get_pieces_array(player_index): get a 2D array of pieces of a player (0 or 1)\n",
    "        \n",
    "        get_board(): get a 2D array of the board with 4 channels (player 0, player 1, empty, and player to move). 4 channels means the array will be of size 4 x n x n\n",
    "    \n",
    "        Descriptions of these methods can be found in the GoState\n",
    "\n",
    "    Input:\n",
    "        game_state: GoState to encode into a fixed size list of features\n",
    "    Output:\n",
    "        features: list of features\n",
    "    \"\"\"\n",
    "    board_size = game_state.size\n",
    "    \n",
    "    # get_piecces_arrary(player_index) -> 2D array (n x n) with 1/0\n",
    "    black = np.array(game_state.get_pieces_array(0), dtype=float)\n",
    "    white = np.array(game_state.get_pieces_array(1), dtype=float)\n",
    "\n",
    "    #empty = 1 - (black + white)\n",
    "    empty = 1.0 - (black + white)\n",
    "    empty = np.clip(empty, 0.0, 1.0)\n",
    "\n",
    "    # playet_to_move\n",
    "    if game_state.player_to_move == 0: # BLACK move\n",
    "        to_move = np.ones((board_size, board_size), dtype=float)\n",
    "    else: # WHITE move\n",
    "        to_move = np.zeros((board_size, board_size), dtype=float)\n",
    "\n",
    "    # Stack channels in order\n",
    "    features = np.stack([black, white, empty, to_move], axis=0)\n",
    "\n",
    "    return features.flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "2cpr86wH8W3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoState(komi=0.5, to_play=W, history.size()=23)\n",
      "\n",
      " 5 +XXX+\n",
      " 4 OOOXX\n",
      " 3 +OXX+\n",
      " 2 OOXX+\n",
      " 1 +X+X+\n",
      "   ABCDE\n",
      "\n",
      "features [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Action # 20\n",
      "Game Result 1.0\n"
     ]
    }
   ],
   "source": [
    "# Print information about first data point\n",
    "data_point = dataset_5x5[0]\n",
    "features = get_features(data_point[0])\n",
    "action = data_point[1]\n",
    "result = data_point[2]\n",
    "print(data_point[0])\n",
    "print(\"features\", features)\n",
    "print(\"Action #\", action)\n",
    "print(\"Game Result\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YI86jYgcOfHC"
   },
   "source": [
    "# Task 2: Supervised Learning of a Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "boPRx0o5Bqq9"
   },
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        # Output is a single scalar value (predicted game outcome from BLACK's perspective)\n",
    "        # Implement a small MLP: input -> 256 -> 128 -> 1\n",
    "        hidden1 = 256\n",
    "        hidden2 = 128\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "      \"\"\"\n",
    "      Run forward pass of network\n",
    "\n",
    "      Input:\n",
    "        x: input to network\n",
    "      Output:\n",
    "        output of network\n",
    "      \"\"\"\n",
    "      # DONE: Update as more layers are added\n",
    "      if not isinstance(x, torch.Tensor):\n",
    "          x = torch.tensor(x, dtype=torch.float32)\n",
    "      # Add batch dimension when given a single example\n",
    "      original_was_1d = False\n",
    "      if x.dim() == 1:\n",
    "          x = x.unsqueeze(0)\n",
    "          original_was_1d = True\n",
    "      out = self.net(x)  # shape (batch, 1)\n",
    "      out = out.squeeze(-1)  # shape (batch,) or scalar if batch==1\n",
    "      if original_was_1d:\n",
    "          # return a scalar tensor (0-d), keep as tensor for consistency with training code\n",
    "          return out.squeeze(0)\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "83a6vGLqB4E7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted Value tensor(-0.0498, grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# This will not produce meaningful outputs until trained, but you can test for syntax errors\n",
    "features_tensor = torch.Tensor(features)\n",
    "value_net = ValueNetwork(len(features))\n",
    "print(\"predicted Value\", value_net(features_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Rq8CokTvOyrI"
   },
   "outputs": [],
   "source": [
    "def train_value_network(dataset, num_epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a value network on the provided dataset.\n",
    "\n",
    "    Input:\n",
    "        dataset: list of (state, action, result) tuples\n",
    "        num_epochs: number of epochs to train for\n",
    "        learning_rate: learning rate for gradient descent\n",
    "    Output:\n",
    "        model: trained model\n",
    "    \"\"\"\n",
    "    # Make sure dataset is shuffled for better performance\n",
    "    random.shuffle(dataset)\n",
    "    # You may find it useful to create train/test sets to better track performance/overfit/underfit\n",
    "    # DONE: Create model\n",
    "    \n",
    "    model = ValueNetwork(input_size=len(get_features(dataset[0][0])))\n",
    "\n",
    "    # DONE: Specify Loss Function\n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "    # You can use Adam, which is stochastic gradient descent with ADAptive Momentum\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_loss = 0\n",
    "        batch_counter = 0\n",
    "        for data_point in dataset:\n",
    "            state = data_point[0]\n",
    "            features = get_features(state)\n",
    "            features_tensor = torch.tensor(features)\n",
    "            \n",
    "\n",
    "            # DONE: What should the desired output of the value network be?\n",
    "            # Note: You will have to convert the label to a torch tensor to use with torch's loss functions\n",
    "            label = torch.tensor([data_point[2]], dtype=torch.float32)\n",
    "            # DONE: Get model prediction of value\n",
    "            prediction = model(features_tensor).squeeze()\n",
    "\n",
    "            # DONE: Compute Loss for data point\n",
    "            loss = loss_function(prediction, label)\n",
    "            batch_loss += loss\n",
    "            batch_counter += 1\n",
    "            if batch_counter % batch_size == 0:\n",
    "                # Call backward to run backward pass and compute gradients\n",
    "                batch_loss.backward()\n",
    "\n",
    "                # Run gradient descent step with optimizer\n",
    "                optimizer.step()\n",
    "\n",
    "                # Reset gradient for next batch\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss = 0\n",
    "\n",
    "    return model\n",
    "\n",
    "value_model = train_value_network(dataset_5x5, 10, 1e-4)\n",
    "save_model(\"value_model.pt\", value_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekP8mzDaTOUM"
   },
   "source": [
    "## Comparing Learned Value function against other Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "UWl3dLOnTbiD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7j/90h8c_f92kl4btc170436vcc0000gn/T/ipykernel_19316/2588542451.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Agent GreedyAgent + Simple Heuristic\n",
      "Learned Agent GreedyAgent + Learned Heuristic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Playing tournament: 100%|██████████| 50/50 [00:01<00:00, 28.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tournament Results\n",
      "Games played: 100\n",
      "GreedyAgent + Learned Heuristic wins: 64 (64.0%)\n",
      "GreedyAgent + Simple Heuristic wins: 36 (36.0%)\n",
      "GreedyAgent + Learned Heuristic wins as BLACK: 39\n",
      "GreedyAgent + Simple Heuristic wins as BLACK: 25\n",
      "GreedyAgent + Learned Heuristic avg move time: 0.001s\n",
      "GreedyAgent + Simple Heuristic avg move time: 0.000s\n",
      "GreedyAgent + Learned Heuristic min time remaining: 26.0s\n",
      "GreedyAgent + Simple Heuristic min time remaining: 26.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TournamentStats(player1_wins=64, player2_wins=36, player1_wins_as_black=39, player2_wins_as_black=25, player1_total_time=np.float64(0.07014208925310815), player2_total_time=np.float64(0.01858210253868624), player1_min_time_remaining=25.98891830444336, player2_min_time_remaining=25.997186183929443, player1_max_move_time=np.float64(0.0051670074462890625), player2_max_move_time=np.float64(0.0010113716125488281), games_played=100)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GoProblemLearnedHeuristic(GoProblem):\n",
    "    def __init__(self, model=None, state=None):\n",
    "        super().__init__(state=state)\n",
    "        self.model = model\n",
    "\n",
    "    def encoding(self, state):\n",
    "        \"\"\"\n",
    "        Get encoding of state (convert state to features)\n",
    "        Note, this may call get_features() from Task 1. \n",
    "\n",
    "        Input:\n",
    "            state: GoState to encode into a fixed size list of features\n",
    "        Output:\n",
    "            features: list of features\n",
    "        \"\"\"\n",
    "        # DONE: get encoding of state (convert state to features)\n",
    "        features = get_features(state)\n",
    "        return features\n",
    "\n",
    "    def heuristic(self, state, player_index):\n",
    "        \"\"\"\n",
    "        Return heuristic (value) of current state\n",
    "\n",
    "        Input:\n",
    "            state: GoState to encode into a fixed size list of features\n",
    "            player_index: index of player to evaluate heuristic for\n",
    "        Output:\n",
    "            value: heuristic (value) of current state\n",
    "        \"\"\"\n",
    "        # DONE: Compute heuristic (value) of current state\n",
    "        if self.model is None:\n",
    "            return 0.0\n",
    "        feats = np.array(self.encoding(state), dtype=np.float32)\n",
    "        x = torch.tensor(feats, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            if x.dim() == 1:\n",
    "                x = x.unsqueeze(0)\n",
    "            pred = self.model(x).squeeze().item()\n",
    "        value = float(pred)\n",
    "\n",
    "        # Note: your agent may perform better if you force it not to pass\n",
    "        # (i.e., don't select action #25 on a 5x5 board unless necessary)\n",
    "        return value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"Learned Heuristic\"\n",
    "\n",
    "def create_value_agent_from_model():\n",
    "    \"\"\"\n",
    "    Create agent object from saved model. This (or other methods like this) will be how your agents will be created in gradescope and in the final tournament.\n",
    "    \"\"\"\n",
    "\n",
    "    model_path = \"value_model.pt\"\n",
    "    # DONE: Update number of features for your own encoding size\n",
    "    sample_state = dataset_5x5[0][0]\n",
    "    feature_size = len(get_features(sample_state))\n",
    "    model = load_model(model_path, ValueNetwork(feature_size))\n",
    "    heuristic_search_problem = GoProblemLearnedHeuristic(model)\n",
    "\n",
    "    # DONE: Try with other heuristic agents (IDS/AB/Minimax)\n",
    "    learned_agent = GreedyAgent(heuristic_search_problem)\n",
    "\n",
    "    return learned_agent\n",
    "\n",
    "# Create agents and run a quick tournament (ensure value_model.pt exists and matches feature size)\n",
    "learned_agent = create_value_agent_from_model()\n",
    "agent2 = GreedyAgent(GoProblemSimpleHeuristic())\n",
    "print(\"Greedy Agent\", agent2)\n",
    "print(\"Learned Agent\", learned_agent)\n",
    "\n",
    "game_runner = GameRunner()\n",
    "game_runner.play_tournament(learned_agent, agent2, num_games=100)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs410_env (3.10.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
